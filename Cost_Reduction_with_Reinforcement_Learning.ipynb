{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cost Reduction with Reinforcement Learning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPdg5WuE5ze/l1HCPI/3Ysr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vbanai/Reinforcement-Learning/blob/main/Cost_Reduction_with_Reinforcement_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project I am just checking how DeepMind AI minimized a big part of Google's cost by reducing\n",
        "Google Data Centre Cooling Bill with using Deep Q-Learning AI model. Here I am experimenting code by Hadelin de Ponteves"
      ],
      "metadata": {
        "id": "nB6-V6EPYwBT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building the Environment in one Class"
      ],
      "metadata": {
        "id": "rzU3SX5YC2yt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMB_K3oJCugI"
      },
      "outputs": [],
      "source": [
        "# Building the Environment\n",
        "\n",
        "# Importing the libraries\n",
        "import numpy as np\n",
        "\n",
        "# BUILDING THE ENVIRONMENT IN A CLASS\n",
        "\n",
        "class Environment(object):\n",
        "    \n",
        "    # INTRODUCING AND INITIALIZING ALL THE PARAMETERS AND VARIABLES OF THE ENVIRONMENT\n",
        "    \n",
        "    def __init__(self, optimal_temperature = (18.0, 24.0), initial_month = 0, initial_number_users = 10, initial_rate_data = 60):\n",
        "        self.monthly_atmospheric_temperatures = [1.0, 5.0, 7.0, 10.0, 11.0, 20.0, 23.0, 24.0, 22.0, 10.0, 5.0, 1.0]\n",
        "        self.initial_month = initial_month\n",
        "        self.atmospheric_temperature = self.monthly_atmospheric_temperatures[initial_month]\n",
        "        self.optimal_temperature = optimal_temperature\n",
        "        self.min_temperature = -20\n",
        "        self.max_temperature = 80\n",
        "        self.min_number_users = 10\n",
        "        self.max_number_users = 100\n",
        "        self.max_update_users = 5\n",
        "        self.min_rate_data = 20\n",
        "        self.max_rate_data = 300\n",
        "        self.max_update_data = 10\n",
        "        self.initial_number_users = initial_number_users\n",
        "        self.current_number_users = initial_number_users\n",
        "        self.initial_rate_data = initial_rate_data\n",
        "        self.current_rate_data = initial_rate_data\n",
        "        self.intrinsic_temperature = self.atmospheric_temperature + 1.25 * self.current_number_users + 1.25 * self.current_rate_data\n",
        "        self.temperature_ai = self.intrinsic_temperature\n",
        "        self.temperature_noai = (self.optimal_temperature[0] + self.optimal_temperature[1]) / 2.0\n",
        "        self.total_energy_ai = 0.0\n",
        "        self.total_energy_noai = 0.0\n",
        "        self.reward = 0.0\n",
        "        self.game_over = 0\n",
        "        self.train = 1\n",
        "\n",
        "    # MAKING A METHOD THAT UPDATES THE ENVIRONMENT RIGHT AFTER THE AI PLAYS AN ACTION\n",
        "    \n",
        "    def update_env(self, direction, energy_ai, month):\n",
        "        \n",
        "        # GETTING THE REWARD\n",
        "        \n",
        "        # Computing the energy spent by the server's cooling system when there is no AI\n",
        "        energy_noai = 0\n",
        "        if (self.temperature_noai < self.optimal_temperature[0]):\n",
        "            energy_noai = self.optimal_temperature[0] - self.temperature_noai\n",
        "            self.temperature_noai = self.optimal_temperature[0]\n",
        "        elif (self.temperature_noai > self.optimal_temperature[1]):\n",
        "            energy_noai = self.temperature_noai - self.optimal_temperature[1]\n",
        "            self.temperature_noai = self.optimal_temperature[1]\n",
        "        # Computing the Reward\n",
        "        self.reward = energy_noai - energy_ai\n",
        "        # Scaling the Reward\n",
        "        self.reward = 1e-3 * self.reward\n",
        "        \n",
        "        # GETTING THE NEXT STATE\n",
        "        \n",
        "        # Updating the atmospheric temperature\n",
        "        self.atmospheric_temperature = self.monthly_atmospheric_temperatures[month]\n",
        "        # Updating the number of users\n",
        "        self.current_number_users += np.random.randint(-self.max_update_users, self.max_update_users)\n",
        "        if (self.current_number_users > self.max_number_users):\n",
        "            self.current_number_users = self.max_number_users\n",
        "        elif (self.current_number_users < self.min_number_users):\n",
        "            self.current_number_users = self.min_number_users\n",
        "        # Updating the rate of data\n",
        "        self.current_rate_data += np.random.randint(-self.max_update_data, self.max_update_data)\n",
        "        if (self.current_rate_data > self.max_rate_data):\n",
        "            self.current_rate_data = self.max_rate_data\n",
        "        elif (self.current_rate_data < self.min_rate_data):\n",
        "            self.current_rate_data = self.min_rate_data\n",
        "        # Computing the Delta of Intrinsic Temperature\n",
        "        past_intrinsic_temperature = self.intrinsic_temperature\n",
        "        self.intrinsic_temperature = self.atmospheric_temperature + 1.25 * self.current_number_users + 1.25 * self.current_rate_data\n",
        "        delta_intrinsic_temperature = self.intrinsic_temperature - past_intrinsic_temperature\n",
        "        # Computing the Delta of Temperature caused by the AI\n",
        "        if (direction == -1):\n",
        "            delta_temperature_ai = -energy_ai\n",
        "        elif (direction == 1):\n",
        "            delta_temperature_ai = energy_ai\n",
        "        # Updating the new Server's Temperature when there is the AI\n",
        "        self.temperature_ai += delta_intrinsic_temperature + delta_temperature_ai\n",
        "        # Updating the new Server's Temperature when there is no AI\n",
        "        self.temperature_noai += delta_intrinsic_temperature\n",
        "        \n",
        "        # GETTING GAME OVER\n",
        "        \n",
        "        if (self.temperature_ai < self.min_temperature):\n",
        "            if (self.train == 1):\n",
        "                self.game_over = 1\n",
        "            else:\n",
        "                self.temperature_ai = self.optimal_temperature[0]\n",
        "                self.total_energy_ai += self.optimal_temperature[0] - self.temperature_ai\n",
        "        elif (self.temperature_ai > self.max_temperature):\n",
        "            if (self.train == 1):\n",
        "                self.game_over = 1\n",
        "            else:\n",
        "                self.temperature_ai = self.optimal_temperature[1]\n",
        "                self.total_energy_ai += self.temperature_ai - self.optimal_temperature[1]\n",
        "        \n",
        "        # UPDATING THE SCORES\n",
        "        \n",
        "        # Updating the Total Energy spent by the AI\n",
        "        self.total_energy_ai += energy_ai\n",
        "        # Updating the Total Energy spent by the server's cooling system when there is no AI\n",
        "        self.total_energy_noai += energy_noai\n",
        "        \n",
        "        # SCALING THE NEXT STATE\n",
        "        \n",
        "        scaled_temperature_ai = (self.temperature_ai - self.min_temperature) / (self.max_temperature - self.min_temperature)\n",
        "        scaled_number_users = (self.current_number_users - self.min_number_users) / (self.max_number_users - self.min_number_users)\n",
        "        scaled_rate_data = (self.current_rate_data - self.min_rate_data) / (self.max_rate_data - self.min_rate_data)\n",
        "        next_state = np.matrix([scaled_temperature_ai, scaled_number_users, scaled_rate_data])\n",
        "        \n",
        "        # RETURNING THE NEXT STATE, THE REWARD, AND GAME OVER\n",
        "        \n",
        "        return next_state, self.reward, self.game_over\n",
        "\n",
        "    # MAKING A METHOD THAT RESETS THE ENVIRONMENT\n",
        "    \n",
        "    def reset(self, new_month):\n",
        "        self.atmospheric_temperature = self.monthly_atmospheric_temperatures[new_month]\n",
        "        self.initial_month = new_month\n",
        "        self.current_number_users = self.initial_number_users\n",
        "        self.current_rate_data = self.initial_rate_data\n",
        "        self.intrinsic_temperature = self.atmospheric_temperature + 1.25 * self.current_number_users + 1.25 * self.current_rate_data\n",
        "        self.temperature_ai = self.intrinsic_temperature\n",
        "        self.temperature_noai = (self.optimal_temperature[0] + self.optimal_temperature[1]) / 2.0\n",
        "        self.total_energy_ai = 0.0\n",
        "        self.total_energy_noai = 0.0\n",
        "        self.reward = 0.0\n",
        "        self.game_over = 0\n",
        "        self.train = 1\n",
        "\n",
        "    # MAKING A METHOD THAT GIVES US AT ANY TIME THE CURRENT STATE, THE LAST REWARD AND WHETHER THE GAME IS OVER\n",
        "    \n",
        "    def observe(self):\n",
        "        scaled_temperature_ai = (self.temperature_ai - self.min_temperature) / (self.max_temperature - self.min_temperature)\n",
        "        scaled_number_users = (self.current_number_users - self.min_number_users) / (self.max_number_users - self.min_number_users)\n",
        "        scaled_rate_data = (self.current_rate_data - self.min_rate_data) / (self.max_rate_data - self.min_rate_data)\n",
        "        current_state = np.matrix([scaled_temperature_ai, scaled_number_users, scaled_rate_data])\n",
        "        return current_state, self.reward, self.game_over\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building the neural network"
      ],
      "metadata": {
        "id": "VA8FhEJAI0Nr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Importing the libraries\n",
        "from keras.layers import Input, Dense, Dropout\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam \n",
        "\n",
        "# BUILDING THE BRAIN\n",
        "\n",
        "class Brain(object):\n",
        "    \n",
        "    # BUILDING A FULLY CONNECTED NEURAL NETWORK DIRECTLY INSIDE THE INIT METHOD\n",
        "    \n",
        "    def __init__(self, learning_rate = 0.001, number_actions = 5):\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        # BUILDIND THE INPUT LAYER COMPOSED OF THE INPUT STATE\n",
        "        states = Input(shape = (3,))\n",
        "        \n",
        "        # BUILDING THE FIRST FULLY CONNECTED HIDDEN LAYER WITH DROPOUT ACTIVATED\n",
        "        x = Dense(units = 64, activation = 'sigmoid')(states)\n",
        "        x = Dropout(rate = 0.1)(x)\n",
        "        \n",
        "        # BUILDING THE SECOND FULLY CONNECTED HIDDEN LAYER WITH DROPOUT ACTIVATED\n",
        "        y = Dense(units = 32, activation = 'sigmoid')(x)\n",
        "        y = Dropout(rate = 0.1)(y)\n",
        "        \n",
        "        # BUILDING THE OUTPUT LAYER, FULLY CONNECTED TO THE LAST HIDDEN LAYER\n",
        "        q_values = Dense(units = number_actions, activation = 'softmax')(y)\n",
        "        \n",
        "        # ASSEMBLING THE FULL ARCHITECTURE INSIDE A MODEL OBJECT\n",
        "        self.model = Model(inputs = states, outputs = q_values)\n",
        "        \n",
        "        # COMPILING THE MODEL WITH A MEAN-SQUARED ERROR LOSS AND A CHOSEN OPTIMIZER\n",
        "        self.model.compile(loss = 'mse', optimizer = Adam(lr = learning_rate))\n"
      ],
      "metadata": {
        "id": "wQA_y8VmIzc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep Q Learning Part"
      ],
      "metadata": {
        "id": "rDJm2uDEJCDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the libraries\n",
        "import numpy as np\n",
        "\n",
        "# IMPLEMENTING DEEP Q-LEARNING WITH EXPERIENCE REPLAY\n",
        "\n",
        "class DQN(object):\n",
        "    \n",
        "    # INTRODUCING AND INITIALIZING ALL THE PARAMETERS AND VARIABLES OF THE DQN\n",
        "    def __init__(self, max_memory = 100, discount = 0.9):\n",
        "        self.memory = list()\n",
        "        self.max_memory = max_memory\n",
        "        self.discount = discount\n",
        "\n",
        "    # MAKING A METHOD THAT BUILDS THE MEMORY IN EXPERIENCE REPLAY\n",
        "    def remember(self, transition, game_over):\n",
        "        self.memory.append([transition, game_over])\n",
        "        if len(self.memory) > self.max_memory:\n",
        "            del self.memory[0]\n",
        "\n",
        "    # MAKING A METHOD THAT BUILDS TWO BATCHES OF INPUTS AND TARGETS BY EXTRACTING TRANSITIONS FROM THE MEMORY\n",
        "    def get_batch(self, model, batch_size = 10):\n",
        "        len_memory = len(self.memory)\n",
        "        num_inputs = self.memory[0][0][0].shape[1]\n",
        "        num_outputs = model.output_shape[-1] #get the last layer shape, to know the number of actions, keras inbuilt api giving output structure\n",
        "        inputs = np.zeros((min(len_memory, batch_size), num_inputs))  #first we initialize everything so 0 is the first figure\n",
        "        targets = np.zeros((min(len_memory, batch_size), num_outputs))\n",
        "        for i, idx in enumerate(np.random.randint(0, len_memory, size = min(len_memory, batch_size))): #take random transitions\n",
        "            current_state, action, reward, next_state = self.memory[idx][0]\n",
        "            game_over = self.memory[idx][1]\n",
        "            inputs[i] = current_state\n",
        "            targets[i] = model.predict(current_state)[0]\n",
        "            Q_sa = np.max(model.predict(next_state)[0])\n",
        "            if game_over:\n",
        "                targets[i, action] = reward\n",
        "            else:\n",
        "                targets[i, action] = reward + self.discount * Q_sa\n",
        "        return inputs, targets"
      ],
      "metadata": {
        "id": "d707-cDOI80a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "vBR1xrzcdDBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the libraries and the other python files\n",
        "import os\n",
        "import numpy as np\n",
        "import random as rn\n",
        "\n",
        "\n",
        "# Setting seeds for reproducibility\n",
        "os.environ['PYTHONHASHSEED'] = '0'\n",
        "np.random.seed(42)\n",
        "rn.seed(12345)\n",
        "\n",
        "# SETTING THE PARAMETERS\n",
        "epsilon = .3\n",
        "number_actions = 5\n",
        "direction_boundary = (number_actions - 1) / 2   #0,1 cooling, 2: nothing, 3,4 heating with 1.5 degree Celsius\n",
        "number_epochs = 100\n",
        "max_memory = 3000\n",
        "batch_size = 512\n",
        "temperature_step = 1.5\n",
        "\n",
        "# BUILDING THE ENVIRONMENT BY SIMPLY CREATING AN OBJECT OF THE ENVIRONMENT CLASS\n",
        "env = Environment(optimal_temperature = (18.0, 24.0), initial_month = 0, initial_number_users = 20, initial_rate_data = 30)\n",
        "\n",
        "# BUILDING THE BRAIN BY SIMPLY CREATING AN OBJECT OF THE BRAIN CLASS\n",
        "brain = Brain(learning_rate = 0.00001, number_actions = number_actions)\n",
        "\n",
        "# BUILDING THE DQN MODEL BY SIMPLY CREATING AN OBJECT OF THE DQN CLASS\n",
        "dqn = DQN(max_memory = max_memory, discount = 0.9)\n",
        "\n",
        "# CHOOSING THE MODE\n",
        "train = True\n",
        "\n",
        "# TRAINING THE AI\n",
        "env.train = train\n",
        "model = brain.model\n",
        "early_stopping = True\n",
        "patience = 10\n",
        "best_total_reward = -np.inf  # to avoid in the beginnig that the cumulative reward will be lower then the initial best t.r value, it is a negative infinity\n",
        "patience_count = 0\n",
        "if (env.train):\n",
        "    # STARTING THE LOOP OVER ALL THE EPOCHS (1 Epoch = 5 Months)\n",
        "    for epoch in range(1, number_epochs):\n",
        "        # INITIALIAZING ALL THE VARIABLES OF BOTH THE ENVIRONMENT AND THE TRAINING LOOP\n",
        "        total_reward = 0\n",
        "        loss = 0.\n",
        "        new_month = np.random.randint(0, 12)\n",
        "        env.reset(new_month = new_month)\n",
        "        game_over = False\n",
        "        current_state, _, _ = env.observe()  #_ just the current state is needed\n",
        "        timestep = 0  #every minute\n",
        "        # STARTING THE LOOP OVER ALL THE TIMESTEPS (1 Timestep = 1 Minute) IN ONE EPOCH\n",
        "        while ((not game_over) and timestep <= 5 * 30 * 24 * 60):   #five months\n",
        "            # PLAYING THE NEXT ACTION BY EXPLORATION\n",
        "            if np.random.rand() <= epsilon:   #gives random number between 0 and 1 rand()   30% of exploration\n",
        "                action = np.random.randint(0, number_actions)\n",
        "                if (action - direction_boundary < 0):\n",
        "                    direction = -1\n",
        "                else:\n",
        "                    direction = 1\n",
        "                energy_ai = abs(action - direction_boundary) * temperature_step\n",
        "            # PLAYING THE NEXT ACTION BY INFERENCE  we use only argmax for business problems, no use of softmax, softmax is used for computer games, when more exploration is needed\n",
        "            else:  #70% exploitation\n",
        "                q_values = model.predict(current_state)\n",
        "                action = np.argmax(q_values[0]) #this gives the column contains all the Q values\n",
        "                if (action - direction_boundary < 0):\n",
        "                    direction = -1\n",
        "                else:\n",
        "                    direction = 1\n",
        "                energy_ai = abs(action - direction_boundary) * temperature_step\n",
        "            # UPDATING THE ENVIRONMENT AND REACHING THE NEXT STATE\n",
        "            next_state, reward, game_over = env.update_env(direction, energy_ai, int(timestep / (30 * 24 * 60))) #az utolsó kiadja, hogy éppen melyik hónapban vagyunk, mert az öt hónapos loop összes percét osztjuk az egy hónap perceivel\n",
        "            total_reward += reward\n",
        "            # STORING THIS NEW TRANSITION INTO THE MEMORY\n",
        "            dqn.remember([current_state, action, reward, next_state], game_over)\n",
        "            # GATHERING IN TWO SEPARATE BATCHES THE INPUTS AND THE TARGETS\n",
        "            inputs, targets = dqn.get_batch(model, batch_size = batch_size)\n",
        "            # COMPUTING THE LOSS OVER THE TWO WHOLE BATCHES OF INPUTS AND TARGETS\n",
        "            loss += model.train_on_batch(inputs, targets)\n",
        "            timestep += 1\n",
        "            current_state = next_state\n",
        "        # PRINTING THE TRAINING RESULTS FOR EACH EPOCH\n",
        "        print(\"\\n\")\n",
        "        print(\"Epoch: {:03d}/{:03d}\".format(epoch, number_epochs))\n",
        "        print(\"Total Energy spent with an AI: {:.0f}\".format(env.total_energy_ai))\n",
        "        print(\"Total Energy spent with no AI: {:.0f}\".format(env.total_energy_noai))\n",
        "        # EARLY STOPPING\n",
        "        if (early_stopping):\n",
        "            if (total_reward <= best_total_reward):\n",
        "                patience_count += 1\n",
        "            elif (total_reward > best_total_reward):\n",
        "                best_total_reward = total_reward\n",
        "                patience_count = 0\n",
        "            if (patience_count >= patience):\n",
        "                print(\"Early Stopping\")\n",
        "                break\n",
        "        # SAVING THE MODEL\n",
        "        model.save(\"model.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3l3CDSSdEt9",
        "outputId": "e4fa4347-e19a-4956-e144-14b6d378a4e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Epoch: 001/100\n",
            "Total Energy spent with an AI: 30\n",
            "Total Energy spent with no AI: 38\n",
            "\n",
            "\n",
            "Epoch: 002/100\n",
            "Total Energy spent with an AI: 21\n",
            "Total Energy spent with no AI: 54\n",
            "\n",
            "\n",
            "Epoch: 003/100\n",
            "Total Energy spent with an AI: 9\n",
            "Total Energy spent with no AI: 0\n",
            "\n",
            "\n",
            "Epoch: 004/100\n",
            "Total Energy spent with an AI: 51\n",
            "Total Energy spent with no AI: 46\n",
            "\n",
            "\n",
            "Epoch: 005/100\n",
            "Total Energy spent with an AI: 28\n",
            "Total Energy spent with no AI: 35\n",
            "\n",
            "\n",
            "Epoch: 006/100\n",
            "Total Energy spent with an AI: 114\n",
            "Total Energy spent with no AI: 141\n",
            "\n",
            "\n",
            "Epoch: 007/100\n",
            "Total Energy spent with an AI: 102\n",
            "Total Energy spent with no AI: 174\n",
            "\n",
            "\n",
            "Epoch: 008/100\n",
            "Total Energy spent with an AI: 99\n",
            "Total Energy spent with no AI: 128\n",
            "\n",
            "\n",
            "Epoch: 009/100\n",
            "Total Energy spent with an AI: 105\n",
            "Total Energy spent with no AI: 104\n",
            "\n",
            "\n",
            "Epoch: 010/100\n",
            "Total Energy spent with an AI: 9\n",
            "Total Energy spent with no AI: 3\n",
            "\n",
            "\n",
            "Epoch: 011/100\n",
            "Total Energy spent with an AI: 6\n",
            "Total Energy spent with no AI: 0\n",
            "\n",
            "\n",
            "Epoch: 012/100\n",
            "Total Energy spent with an AI: 3\n",
            "Total Energy spent with no AI: 0\n",
            "\n",
            "\n",
            "Epoch: 013/100\n",
            "Total Energy spent with an AI: 34\n",
            "Total Energy spent with no AI: 66\n",
            "\n",
            "\n",
            "Epoch: 014/100\n",
            "Total Energy spent with an AI: 14\n",
            "Total Energy spent with no AI: 16\n",
            "\n",
            "\n",
            "Epoch: 015/100\n",
            "Total Energy spent with an AI: 22\n",
            "Total Energy spent with no AI: 38\n",
            "\n",
            "\n",
            "Epoch: 016/100\n",
            "Total Energy spent with an AI: 60\n",
            "Total Energy spent with no AI: 52\n",
            "\n",
            "\n",
            "Epoch: 017/100\n",
            "Total Energy spent with an AI: 14\n",
            "Total Energy spent with no AI: 12\n",
            "Early Stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing"
      ],
      "metadata": {
        "id": "Hg5w9nmISUdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random as rn\n",
        "from keras.models import load_model\n",
        "\n",
        "\n",
        "# Setting seeds for reproducibility\n",
        "os.environ['PYTHONHASHSEED'] = '0'\n",
        "np.random.seed(42)\n",
        "rn.seed(12345)\n",
        "\n",
        "# SETTING THE PARAMETERS\n",
        "number_actions = 5\n",
        "direction_boundary = (number_actions - 1) / 2\n",
        "temperature_step = 1.5\n",
        "\n",
        "# BUILDING THE ENVIRONMENT BY SIMPLY CREATING AN OBJECT OF THE ENVIRONMENT CLASS\n",
        "env = Environment(optimal_temperature = (18.0, 24.0), initial_month = 0, initial_number_users = 20, initial_rate_data = 30)\n",
        "\n",
        "# LOADING A PRE-TRAINED BRAIN\n",
        "model = load_model(\"model.h5\")\n",
        "\n",
        "# CHOOSING THE MODE\n",
        "train = False\n",
        "\n",
        "# RUNNING A 1 YEAR SIMULATION IN INFERENCE MODE\n",
        "env.train = train\n",
        "current_state, _, _ = env.observe()\n",
        "for timestep in range(0, 12 * 30 * 24 * 60):\n",
        "    q_values = model.predict(current_state)\n",
        "    action = np.argmax(q_values[0])\n",
        "    if (action - direction_boundary < 0):\n",
        "        direction = -1\n",
        "    else:\n",
        "        direction = 1\n",
        "    energy_ai = abs(action - direction_boundary) * temperature_step\n",
        "    next_state, reward, game_over = env.update_env(direction, energy_ai, int(timestep / (30 * 24 * 60)))\n",
        "    current_state = next_state\n",
        "\n",
        "# PRINTING THE TRAINING RESULTS FOR EACH EPOCH\n",
        "print(\"\\n\")\n",
        "print(\"Total Energy spent with an AI: {:.0f}\".format(env.total_energy_ai))\n",
        "print(\"Total Energy spent with no AI: {:.0f}\".format(env.total_energy_noai))\n",
        "print(\"ENERGY SAVED: {:.0f} %\".format((env.total_energy_noai - env.total_energy_ai) / env.total_energy_noai * 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q79kLcm77djx",
        "outputId": "df1d6a42-f9ce-42fb-b129-6b8f50c8821d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Total Energy spent with an AI: 1555200\n",
            "Total Energy spent with no AI: 1978296\n",
            "ENERGY SAVED: 21 %\n"
          ]
        }
      ]
    }
  ]
}