{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vbanai/Reinforcement-Learning/blob/main/TD3_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXu1r8qvSzWf"
      },
      "source": [
        "TWIN-DELAYED DDPG (Deep deterministic policy gradient)</br>\n",
        "</br>\n",
        "The Twin-Delayed3 is applied to environment having a continous action space, which means that the AI is\n",
        "playing some continuous actions in a range of continous values. Here I am just checking and experimenting with the hyper parameters of the leading - edge TD3 model on some open AI gym applications"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAHMB0Ze8fU0",
        "outputId": "4abb6a2b-a871-4cb9-9fdc-1a81a5350917"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pybullet in /usr/local/lib/python3.7/dist-packages (3.2.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install pybullet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ikr2p0Js8iB4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pybullet_envs\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from gym import wrappers\n",
        "from torch.autograd import Variable\n",
        "from collections import deque"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2nGdtlKVydr"
      },
      "source": [
        "Step 1: Initializing the Experience Replay memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "u5rW0IDB8nTO"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer(object):\n",
        "\n",
        "  def __init__(self, max_size=1e6):\n",
        "    self.storage = []\n",
        "    self.max_size = max_size\n",
        "    self.ptr = 0  # indexing the memory cells\n",
        "\n",
        "  def add(self, transition):\n",
        "    if len(self.storage) == self.max_size:\n",
        "      self.storage[int(self.ptr)] = transition\n",
        "      self.ptr = (self.ptr + 1) % self.max_size\n",
        "    else:\n",
        "      self.storage.append(transition)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
        "    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
        "    for i in ind: \n",
        "      state, next_state, action, reward, done = self.storage[i]\n",
        "      batch_states.append(np.array(state, copy=False))\n",
        "      batch_next_states.append(np.array(next_state, copy=False))\n",
        "      batch_actions.append(np.array(action, copy=False))\n",
        "      batch_rewards.append(np.array(reward, copy=False))\n",
        "      batch_dones.append(np.array(done, copy=False))\n",
        "    return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jb7TTaHxWbQD"
      },
      "source": [
        "Step 2: Actor model and the Actor target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4CeRW4D79HL0"
      },
      "outputs": [],
      "source": [
        "class Actor(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):  #actor model play several action at the same time which is defined by the action dim dimension\n",
        "    super(Actor, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action * torch.tanh(self.layer_3(x))\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRDDce8FXef7"
      },
      "source": [
        "Step 3: Critic model and the critic target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OCee7gwR9Jrs"
      },
      "outputs": [],
      "source": [
        "class Critic(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim):   #state_dim coordinates of the robot  , action _dim how many actions the agent will play at the same time\n",
        "    super(Critic, self).__init__()\n",
        "    # Defining the first Critic neural network\n",
        "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, 1)\n",
        "    # Defining the second Critic neural network\n",
        "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, 1)\n",
        "\n",
        "  def forward(self, x, u):  # we have two inputs state and action which should be concat\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    # Forward-Propagation on the first Critic Neural Network\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    # Forward-Propagation on the second Critic Neural Network\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "    return x1, x2\n",
        "\n",
        "  def Q1(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    return x1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzIDuONodenW"
      },
      "source": [
        "Steps 4: Training Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zzd0H1xukdKe"
      },
      "outputs": [],
      "source": [
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Building the whole Training Process into a class\n",
        "\n",
        "class TD3(object):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):  #it can train AI on any environment\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())  #performing gradient ascent in order to update of the weights of the actor to increase the output Q value of the CRITIC\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state.reshape(1, -1)).to(device)  #state needs to be one dimentional horizontally\n",
        "    return self.actor(state).cpu().data.numpy().flatten()  #state is forward propagated through the neural network to get the action, and it is not so computedemanding step that is why cpu can be used \n",
        "                                                           #converting tensor to numpy array agan 1D that is why using flattening, as we will clip and add noise to the action and we do it when its format is numpy\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "      #tau for polyac averaging  , policy noise for exploration adding to the action and then clip it, it is the standard dev in the gaussian dist, policy freq is the delay once in \"2\" iteration\n",
        "    for it in range(iterations): \n",
        "      \n",
        "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "      \n",
        "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
        "      next_action = self.actor_target(next_state) #forward also can be used with actor_target.forward()\n",
        "      \n",
        "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)  #0 is the mean and policy n is the stand.dev\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)  #giving the range between noise is clipped, clamp is clipping\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "      \n",
        "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "      \n",
        "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "      \n",
        "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach() #1-done is needed as we have to annualate the targetQ when we are at the end of the episode as no further Qtarget\n",
        "                                          # tensors belongs to computational graph which makes the computation much more efficient when calculationg gradinents, so to add target to reward (which is not in the comp graph), we have to detach the targetQ from the graph      \n",
        "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "      \n",
        "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "      \n",
        "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "      \n",
        "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        \n",
        "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "        \n",
        "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "  \n",
        "  # Making a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "  \n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka-ZRtQvjBex"
      },
      "source": [
        "Step 5: Function that evaluates the policy by calculating its average reward over 10 episodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qabqiYdp9wDM"
      },
      "outputs": [],
      "source": [
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(np.array(obs))\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGuKmH_ijf7U"
      },
      "source": [
        "Step 6: Setting the parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HFj6wbAo97lk"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "env_name=\"AntBulletEnv-v0\"    #env_name = \"MountainCarContinuous-v0\"            # Name of a environment (set it to any Continous environment you want)\n",
        "seed = 0 # Random seed number\n",
        "start_timesteps = 5e4 #1e4 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network, it is for exploration after this, expoitation comes when  the agent playes 10.000 episodes \n",
        "eval_freq = 5e3 # How often the evaluation step is performed (after how many timesteps) after 5e time steps\n",
        "max_timesteps =1e5# 5e5 # Total number of iterations/timesteps, it can be varried, I tried 100 000, 500 000, 800 000 for different applications\n",
        "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
        "expl_noise = 0.1 # Exploration noise - STD value of exploration Gaussian noise\n",
        "batch_size = 100 # Size of the batch\n",
        "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
        "tau = 0.005 # Target network update rate\n",
        "policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes\n",
        "noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n",
        "policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hjwf2HCol3XP"
      },
      "source": [
        "Step 7: Creating a file name for the two saved models (saved weights): the Actor and Critic models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fyH8N5z-o3o",
        "outputId": "0614c46a-e3f5-4731-8fb8-e595e162a1af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------\n",
            "Settings: TD3_AntBulletEnv-v0_0\n",
            "---------------------------------------\n"
          ]
        }
      ],
      "source": [
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kop-C96Aml8O"
      },
      "source": [
        "Step 10: We create a folder inside which will be saved the trained models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Src07lvY-zXb"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"./results\"):\n",
        "  os.makedirs(\"./results\")\n",
        "if save_models and not os.path.exists(\"./pytorch_models\"):\n",
        "  os.makedirs(\"./pytorch_models\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEAzOd47mv1Z"
      },
      "source": [
        "Step 9: Creating the PyBullet environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "CyQXJUIs-6BV"
      },
      "outputs": [],
      "source": [
        "env = gym.make(env_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YdPG4HXnNsh"
      },
      "source": [
        "Step 10: We set seeds and we get the necessary information on the states and actions in the chosen environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Z3RufYec_ADj"
      },
      "outputs": [],
      "source": [
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0] #automatically we call the observation space from the environment\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWEgDAQxnbem"
      },
      "source": [
        "Step 11: Creating the policy network (the Actor model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "wTVvG7F8_EWg"
      },
      "outputs": [],
      "source": [
        "policy = TD3(state_dim, action_dim, max_action)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI60VN2Unklh"
      },
      "source": [
        "Step 12: Creating the Experience Replay memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "sd-ZsdXR_LgV"
      },
      "outputs": [],
      "source": [
        "replay_buffer = ReplayBuffer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYOpCyiDnw7s"
      },
      "source": [
        "Step 13: Defining a list where all the evaluation results over 10 episodes are stored"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhC_5XJ__Orp",
        "outputId": "315b4774-0197-4385-d830-37a0a455dd5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 9.804960\n",
            "---------------------------------------\n"
          ]
        }
      ],
      "source": [
        "evaluations = [evaluate_policy(policy)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm-4b3p6rglE"
      },
      "source": [
        "Step 13: Creating a new folder directory in which the final results (videos of the agent) will be populated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "MTL9uMd0ru03"
      },
      "outputs": [],
      "source": [
        "def mkdir(base, name):\n",
        "    path = os.path.join(base, name)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    return path\n",
        "work_dir = mkdir('exp', 'brs')\n",
        "monitor_dir = mkdir(work_dir, 'monitor')\n",
        "max_episode_steps = env._max_episode_steps\n",
        "save_env_vid = False\n",
        "if save_env_vid:\n",
        "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "  env.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31n5eb03p-Fm"
      },
      "source": [
        "Step 14: Initializing the variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "1vN5EvxK_QhT"
      },
      "outputs": [],
      "source": [
        "total_timesteps = 0\n",
        "timesteps_since_eval = 0\n",
        "episode_num = 0\n",
        "done = True\n",
        "t0 = time.time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9gsjvtPqLgT"
      },
      "source": [
        "Step 15: Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_ouY4NH_Y0I",
        "outputId": "07c69f7f-2419-4cc5-d8f3-d61da98ff347"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Timesteps: 1000 Episode Num: 1 Reward: 499.187467768905\n",
            "Total Timesteps: 1315 Episode Num: 2 Reward: 141.3908612591943\n",
            "Total Timesteps: 2315 Episode Num: 3 Reward: 481.43126230095777\n",
            "Total Timesteps: 3315 Episode Num: 4 Reward: 483.4682574094196\n",
            "Total Timesteps: 4315 Episode Num: 5 Reward: 382.16776603577773\n",
            "Total Timesteps: 5315 Episode Num: 6 Reward: 481.3461168336129\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 112.385621\n",
            "---------------------------------------\n",
            "Total Timesteps: 6315 Episode Num: 7 Reward: 490.4055357124045\n",
            "Total Timesteps: 7315 Episode Num: 8 Reward: 422.7945637944035\n",
            "Total Timesteps: 7934 Episode Num: 9 Reward: 270.7201179925483\n",
            "Total Timesteps: 8934 Episode Num: 10 Reward: 270.14830334888615\n",
            "Total Timesteps: 8976 Episode Num: 11 Reward: 15.275823524710722\n",
            "Total Timesteps: 9976 Episode Num: 12 Reward: 524.55231403633\n",
            "Total Timesteps: 10976 Episode Num: 13 Reward: 203.24211364457273\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 184.406095\n",
            "---------------------------------------\n",
            "Total Timesteps: 11976 Episode Num: 14 Reward: 182.9432872964881\n",
            "Total Timesteps: 12976 Episode Num: 15 Reward: 197.53821081482158\n",
            "Total Timesteps: 13821 Episode Num: 16 Reward: 69.1557737889739\n",
            "Total Timesteps: 14821 Episode Num: 17 Reward: 156.10244143792897\n",
            "Total Timesteps: 15821 Episode Num: 18 Reward: 144.04141089711317\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 91.618946\n",
            "---------------------------------------\n",
            "Total Timesteps: 16591 Episode Num: 19 Reward: 82.80459930857799\n",
            "Total Timesteps: 17188 Episode Num: 20 Reward: 78.72012934210764\n",
            "Total Timesteps: 17674 Episode Num: 21 Reward: 34.85472990640893\n",
            "Total Timesteps: 18674 Episode Num: 22 Reward: 94.1335347778862\n",
            "Total Timesteps: 19674 Episode Num: 23 Reward: 90.56076227862816\n",
            "Total Timesteps: 20674 Episode Num: 24 Reward: 100.6716111632991\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -3.834932\n",
            "---------------------------------------\n",
            "Total Timesteps: 20705 Episode Num: 25 Reward: -4.411354411975778\n",
            "Total Timesteps: 21705 Episode Num: 26 Reward: 192.90375076978634\n",
            "Total Timesteps: 21763 Episode Num: 27 Reward: 13.297012928785982\n",
            "Total Timesteps: 22763 Episode Num: 28 Reward: 325.758164556976\n",
            "Total Timesteps: 23763 Episode Num: 29 Reward: 140.36796122975778\n",
            "Total Timesteps: 24763 Episode Num: 30 Reward: 303.47193150254117\n",
            "Total Timesteps: 25763 Episode Num: 31 Reward: 98.0152793708246\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 176.440231\n",
            "---------------------------------------\n",
            "Total Timesteps: 25887 Episode Num: 32 Reward: 48.70775765024092\n",
            "Total Timesteps: 26048 Episode Num: 33 Reward: 106.24672286453402\n",
            "Total Timesteps: 26069 Episode Num: 34 Reward: 1.4135764567973639\n",
            "Total Timesteps: 26300 Episode Num: 35 Reward: 101.65127093001448\n",
            "Total Timesteps: 27300 Episode Num: 36 Reward: 348.77869803923704\n",
            "Total Timesteps: 27331 Episode Num: 37 Reward: 15.52198795626809\n",
            "Total Timesteps: 27363 Episode Num: 38 Reward: 16.880036314636094\n",
            "Total Timesteps: 28363 Episode Num: 39 Reward: 236.52826283741393\n",
            "Total Timesteps: 29363 Episode Num: 40 Reward: 320.0170407151732\n",
            "Total Timesteps: 30363 Episode Num: 41 Reward: 470.82588472268424\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 274.152268\n",
            "---------------------------------------\n",
            "Total Timesteps: 31363 Episode Num: 42 Reward: 420.5975900311664\n",
            "Total Timesteps: 32363 Episode Num: 43 Reward: 317.9144083127172\n",
            "Total Timesteps: 33363 Episode Num: 44 Reward: 194.42442917095485\n",
            "Total Timesteps: 34363 Episode Num: 45 Reward: 503.3925687932081\n",
            "Total Timesteps: 35363 Episode Num: 46 Reward: 263.6042059888424\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 263.003819\n",
            "---------------------------------------\n",
            "Total Timesteps: 36363 Episode Num: 47 Reward: 320.76442010919925\n",
            "Total Timesteps: 37363 Episode Num: 48 Reward: 265.1506382412925\n",
            "Total Timesteps: 38363 Episode Num: 49 Reward: 371.52071785568216\n",
            "Total Timesteps: 39363 Episode Num: 50 Reward: 167.4602748355808\n",
            "Total Timesteps: 40363 Episode Num: 51 Reward: 231.70965661734687\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 335.206564\n",
            "---------------------------------------\n",
            "Total Timesteps: 41363 Episode Num: 52 Reward: 283.68744099479784\n",
            "Total Timesteps: 42363 Episode Num: 53 Reward: 296.0369605463688\n",
            "Total Timesteps: 42525 Episode Num: 54 Reward: 85.41055697623321\n",
            "Total Timesteps: 42546 Episode Num: 55 Reward: 3.915108424317929\n",
            "Total Timesteps: 43546 Episode Num: 56 Reward: 639.0610142952518\n",
            "Total Timesteps: 44320 Episode Num: 57 Reward: 198.9009042722917\n",
            "Total Timesteps: 45320 Episode Num: 58 Reward: 377.36564785958086\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 376.242350\n",
            "---------------------------------------\n",
            "Total Timesteps: 46320 Episode Num: 59 Reward: 358.7472023454961\n",
            "Total Timesteps: 47320 Episode Num: 60 Reward: 218.05665683652865\n",
            "Total Timesteps: 48320 Episode Num: 61 Reward: 363.5196907479656\n",
            "Total Timesteps: 49320 Episode Num: 62 Reward: 376.27489101490477\n",
            "Total Timesteps: 50320 Episode Num: 63 Reward: 197.00280234134513\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 363.714382\n",
            "---------------------------------------\n",
            "Total Timesteps: 51320 Episode Num: 64 Reward: 611.9129365939845\n",
            "Total Timesteps: 52320 Episode Num: 65 Reward: 368.53210226423624\n",
            "Total Timesteps: 53320 Episode Num: 66 Reward: 553.8455530295064\n",
            "Total Timesteps: 53551 Episode Num: 67 Reward: 22.837400718967295\n",
            "Total Timesteps: 54551 Episode Num: 68 Reward: 333.02631804970713\n",
            "Total Timesteps: 55551 Episode Num: 69 Reward: 298.71056834108555\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 399.003500\n",
            "---------------------------------------\n",
            "Total Timesteps: 56551 Episode Num: 70 Reward: 353.8163826798814\n",
            "Total Timesteps: 57551 Episode Num: 71 Reward: 528.6822268795144\n",
            "Total Timesteps: 58551 Episode Num: 72 Reward: 339.4247558455058\n",
            "Total Timesteps: 58572 Episode Num: 73 Reward: 0.5541823975365721\n",
            "Total Timesteps: 58592 Episode Num: 74 Reward: 1.5073067575382533\n",
            "Total Timesteps: 58613 Episode Num: 75 Reward: 0.8977147186508705\n",
            "Total Timesteps: 58634 Episode Num: 76 Reward: 1.1044952808861552\n",
            "Total Timesteps: 58654 Episode Num: 77 Reward: 1.540194882743498\n",
            "Total Timesteps: 58674 Episode Num: 78 Reward: 1.4400333266214043\n",
            "Total Timesteps: 59296 Episode Num: 79 Reward: 198.91767064691987\n",
            "Total Timesteps: 60296 Episode Num: 80 Reward: 448.53394388092306\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 452.775944\n",
            "---------------------------------------\n",
            "Total Timesteps: 61296 Episode Num: 81 Reward: 336.1232108706142\n",
            "Total Timesteps: 62296 Episode Num: 82 Reward: 323.74504178158367\n",
            "Total Timesteps: 63296 Episode Num: 83 Reward: 481.71296797321935\n",
            "Total Timesteps: 64296 Episode Num: 84 Reward: 139.01731308869645\n",
            "Total Timesteps: 65296 Episode Num: 85 Reward: 593.6239196734342\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 454.553622\n",
            "---------------------------------------\n",
            "Total Timesteps: 66296 Episode Num: 86 Reward: 536.1127556394806\n",
            "Total Timesteps: 67296 Episode Num: 87 Reward: 356.214086587413\n",
            "Total Timesteps: 68296 Episode Num: 88 Reward: 543.9894406758625\n",
            "Total Timesteps: 69296 Episode Num: 89 Reward: 571.1636225976077\n",
            "Total Timesteps: 70296 Episode Num: 90 Reward: 422.38526891714224\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 402.815894\n",
            "---------------------------------------\n",
            "Total Timesteps: 71296 Episode Num: 91 Reward: 412.36746731694853\n",
            "Total Timesteps: 72296 Episode Num: 92 Reward: 426.3763478591361\n",
            "Total Timesteps: 73296 Episode Num: 93 Reward: 363.6564770814326\n",
            "Total Timesteps: 74296 Episode Num: 94 Reward: 245.3442164515794\n",
            "Total Timesteps: 75296 Episode Num: 95 Reward: 436.7349477109873\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 567.440835\n",
            "---------------------------------------\n",
            "Total Timesteps: 76296 Episode Num: 96 Reward: 581.5313490338135\n",
            "Total Timesteps: 77296 Episode Num: 97 Reward: 378.7191447136329\n",
            "Total Timesteps: 78296 Episode Num: 98 Reward: 477.1102132338817\n",
            "Total Timesteps: 79296 Episode Num: 99 Reward: 450.59928101164724\n",
            "Total Timesteps: 80296 Episode Num: 100 Reward: 568.5752364982962\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 519.705505\n",
            "---------------------------------------\n",
            "Total Timesteps: 81296 Episode Num: 101 Reward: 524.240149169054\n",
            "Total Timesteps: 82296 Episode Num: 102 Reward: 559.946601875315\n",
            "Total Timesteps: 82316 Episode Num: 103 Reward: 4.02667618803072\n",
            "Total Timesteps: 83316 Episode Num: 104 Reward: 535.5257188801659\n",
            "Total Timesteps: 83336 Episode Num: 105 Reward: 5.036028848920638\n",
            "Total Timesteps: 83356 Episode Num: 106 Reward: 5.502841841452435\n",
            "Total Timesteps: 83376 Episode Num: 107 Reward: 5.040404690541439\n",
            "Total Timesteps: 83396 Episode Num: 108 Reward: 5.76183563527009\n",
            "Total Timesteps: 83416 Episode Num: 109 Reward: 3.650842725927506\n",
            "Total Timesteps: 83436 Episode Num: 110 Reward: 5.181338306576958\n",
            "Total Timesteps: 83848 Episode Num: 111 Reward: 255.6887474459756\n",
            "Total Timesteps: 84848 Episode Num: 112 Reward: 229.46893356436658\n",
            "Total Timesteps: 85848 Episode Num: 113 Reward: 505.59835181781136\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 564.106385\n",
            "---------------------------------------\n",
            "Total Timesteps: 86848 Episode Num: 114 Reward: 385.82150790514663\n",
            "Total Timesteps: 87848 Episode Num: 115 Reward: 274.04987664193936\n",
            "Total Timesteps: 88848 Episode Num: 116 Reward: 484.54569888444354\n",
            "Total Timesteps: 89848 Episode Num: 117 Reward: 117.87891163944097\n",
            "Total Timesteps: 90848 Episode Num: 118 Reward: 419.84321407779163\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 265.126547\n",
            "---------------------------------------\n",
            "Total Timesteps: 91848 Episode Num: 119 Reward: 298.2607651679115\n",
            "Total Timesteps: 92848 Episode Num: 120 Reward: 375.15705264919575\n",
            "Total Timesteps: 93848 Episode Num: 121 Reward: 480.39894147464895\n",
            "Total Timesteps: 94848 Episode Num: 122 Reward: 611.8202939167984\n",
            "Total Timesteps: 95848 Episode Num: 123 Reward: 720.6032913238911\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 347.069030\n",
            "---------------------------------------\n",
            "Total Timesteps: 96848 Episode Num: 124 Reward: 262.6305148411139\n",
            "Total Timesteps: 97848 Episode Num: 125 Reward: 349.6292949773361\n",
            "Total Timesteps: 98848 Episode Num: 126 Reward: 383.94013119951774\n",
            "Total Timesteps: 99848 Episode Num: 127 Reward: 464.7443652238083\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 474.269530\n",
            "---------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# We start the main loop over 500,000 timesteps\n",
        "while total_timesteps < max_timesteps:\n",
        "  \n",
        "  # If the episode is done\n",
        "  if done:\n",
        "\n",
        "    # If we are not at the very beginning, we start the training process of the model\n",
        "    if total_timesteps != 0:\n",
        "      print(\"Total Timesteps: {} Episode Num: {} Reward: {}\".format(total_timesteps, episode_num, episode_reward))\n",
        "      policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
        "\n",
        "    # We evaluate the episode and we save the policy\n",
        "    if timesteps_since_eval >= eval_freq:\n",
        "      timesteps_since_eval %= eval_freq\n",
        "      evaluations.append(evaluate_policy(policy))\n",
        "      policy.save(file_name, directory=\"./pytorch_models\")\n",
        "      np.save(\"./results/%s\" % (file_name), evaluations)\n",
        "    \n",
        "    # When the training step is done, we reset the state of the environment\n",
        "    obs = env.reset()\n",
        "    \n",
        "    # Set the Done to False\n",
        "    done = False\n",
        "    \n",
        "    # Set rewards and episode timesteps to zero\n",
        "    episode_reward = 0\n",
        "    episode_timesteps = 0\n",
        "    episode_num += 1\n",
        "  \n",
        "  # Before 10000 timesteps, we play random actions\n",
        "  if total_timesteps < start_timesteps:\n",
        "    action = env.action_space.sample()\n",
        "  else: # After 10000 timesteps, we switch to the model\n",
        "    action = policy.select_action(np.array(obs))\n",
        "    # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n",
        "    if expl_noise != 0:\n",
        "      action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n",
        "  \n",
        "  # The agent performs the action in the environment, then reaches the next state and receives the reward\n",
        "  new_obs, reward, done, _ = env.step(action)\n",
        "  \n",
        "  # We check if the episode is done\n",
        "  done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n",
        "  \n",
        "  # We increase the total reward\n",
        "  episode_reward += reward\n",
        "  \n",
        "  # We store the new transition into the Experience Replay memory (ReplayBuffer)\n",
        "  replay_buffer.add((obs, new_obs, action, reward, done_bool))\n",
        "\n",
        "  # We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n",
        "  obs = new_obs\n",
        "  episode_timesteps += 1\n",
        "  total_timesteps += 1\n",
        "  timesteps_since_eval += 1\n",
        "\n",
        "# We add the last policy evaluation to our list of evaluations and we save our model\n",
        "evaluations.append(evaluate_policy(policy))\n",
        "if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
        "np.save(\"./results/%s\" % (file_name), evaluations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi6e2-_pu05e"
      },
      "source": [
        "Step 16: Inference, when we play in the environment using the trained weights in the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "oW4d1YAMqif1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d471ea15-a9a0-4b2d-c035-ef8bdef431ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------\n",
            "Settings: TD3_AntBulletEnv-v0_0\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 509.188821\n",
            "---------------------------------------\n"
          ]
        }
      ],
      "source": [
        "class Actor(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action * torch.tanh(self.layer_3(x)) \n",
        "    return x\n",
        "\n",
        "class Critic(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    # Defining the first Critic neural network\n",
        "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, 1)\n",
        "    # Defining the second Critic neural network\n",
        "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, 1)\n",
        "\n",
        "  def forward(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    # Forward-Propagation on the first Critic Neural Network\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    # Forward-Propagation on the second Critic Neural Network\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "    return x1, x2\n",
        "\n",
        "  def Q1(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    return x1\n",
        "\n",
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Building the whole Training Process into a class\n",
        "\n",
        "class TD3(object):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "    \n",
        "    for it in range(iterations):\n",
        "      \n",
        "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "      \n",
        "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
        "      next_action = self.actor_target(next_state)\n",
        "      \n",
        "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "      \n",
        "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "      \n",
        "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "      \n",
        "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "      \n",
        "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "      \n",
        "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "      \n",
        "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "      \n",
        "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        \n",
        "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "        \n",
        "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "  \n",
        "  # Making a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "  \n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))\n",
        "\n",
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(np.array(obs))\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward\n",
        "\n",
        "env_name = \"AntBulletEnv-v0\" #\"MountainCarContinuous-v0\"\n",
        "seed = 0\n",
        "\n",
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")\n",
        "\n",
        "eval_episodes = 10\n",
        "save_env_vid = True\n",
        "env = gym.make(env_name)\n",
        "max_episode_steps = env._max_episode_steps\n",
        "if save_env_vid:\n",
        "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "  env.reset()\n",
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])\n",
        "policy = TD3(state_dim, action_dim, max_action)\n",
        "policy.load(file_name, './pytorch_models/')\n",
        "_ = evaluate_policy(policy, eval_episodes=eval_episodes)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "TD3_model.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}